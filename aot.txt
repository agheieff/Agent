Atom of Thoughts for Markov LLM Test-Time Scaling
Fengwei Teng1,2  Zhaoyang Yu2  Quan Shi3  Jiayi Zhang1,2
Chenglin Wu2  Yuyu Luo1
1The Hong Kong University of Science and Technology (Guangzhou)
2DeepWisdom  3Renmin University of China   Corresponding authors.
Abstract

Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning progress is often achieved by solving a sequence of independent subquestions, each being self-contained and verifiable. These subquestions are essentially atomic questions, relying primarily on their current state rather than accumulated history, similar to the memoryless transitions in a Markov process. Based on this observation, we propose Atom of Thoughts (AoT), where each state transition in the reasoning process consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a new atomic question state. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, naturally realizing Markov transitions between question states. Furthermore, these atomic questions can be seamlessly integrated into existing test-time scaling methods, enabling AoT to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, AoT achieves an 80.6% F1 score, surpassing o3-mini by 3.4% and DeepSeek-R1 by 10.6%. The code will be available at https://github.com/qixucen/atom.

Atom of Thoughts for Markov LLM Test-Time Scaling

Fengwei Teng1,2  Zhaoyang Yu2  Quan Shi3  Jiayi Zhang1,2 Chenglin Wu†2  Yuyu Luo1 1The Hong Kong University of Science and Technology (Guangzhou) 2DeepWisdom  3Renmin University of China

Refer to caption
Figure 1: Comparison of computational resource allocation in test-time scaling methods. Traditional test-time scaling methods allocate computational resources partially to process historical information, while AoT dedicates all computational resources to reasoning directly related to the current atomic question state.
Refer to caption
Figure 2: The overview of AoT. The left portion illustrates our Markov process where each state Qi represents an atomic reasoning state derived through DAG decomposition and contraction from its predecessor. The right portion demonstrates AoT’s integration capability with existing test-time scaling methods (e.g., CoT, ToT). A key feature of this integration is that any intermediate state Qi from our Markov process can serve as an entry point (Q0) for other methods, enabling flexible composition while maintaining answer equivalence with the original question. This design allows AoT to function both as a standalone iterative framework and as a preprocessing module that can enhance existing approaches through structural optimization.
1 Introduction

Large Language Models (LLMs) demonstrate significant scaling effects, with their capabilities showing predictable improvements as model parameters and training data increase, leading to enhanced performance across diverse domains Kaplan et al. (2020). While this scaling law faces bottlenecks in high-quality data availability, test-time scaling offers an alternative solution by forcing LLMs to engage in effective logical reasoning during inference to improve performance on diverse tasks Snell et al. (2024); Muennighoff et al. (2025); Hou et al. (2025); Zhang et al. (2024a).

However, existing test-time scaling methods excessively maintain historical information during reasoning, as they rely heavily on complex structural dependencies throughout the reasoning process. Chain-based methods must preserve the entire reasoning history to generate each subsequent step Wei et al. (2022); Zhang et al. (2023), while tree-based approaches require tracking both ancestor and sibling relationships for branch selection Yao et al. (2023); Zhou et al. (2024); Ding et al. (2024). Graph-based structures further compound these challenges through arbitrary node dependencies. As the scale of reasoning increases, the accumulation of historical dependencies not only wastes substantial computational resources but also interferes with the model’s ability to reason effectively, as illustrated in Figure 1.

Human reasoning often progresses through solving a sequence of independent subquestions, a fundamental principle established in cognitive science Simon (1962) and problem-solving theory Polya (1945). When solving a complex problem, we naturally identify and resolve self-evident subquestions first, then seamlessly incorporate these solutions to reformulate a simplified problem state, rather than maintaining detailed reasoning processes for resolved components. This progression closely resembles a Markov process Markov (1906), where each state represents a question, and state transitions occur through resolving partial problems to form new, independent questions.

Inspired by this Markov nature of human reasoning, we propose Atom of Thoughts (AoT), a framework that realizes the Markov-style reasoning process. Our key insight is that each reasoning state can be defined as a simplified problem equivalent to the original one, where partial reasoning steps are either transformed into known conditions or excluded as incorrect explorations. This definition is achieved through a two-phase state transition mechanism: first decomposing the current question into a dependency-based directed acyclic graph (DAG) to capture rich structural information, then contracting subquestions into a new independent question. This iterative decomposition-contraction process continues until reaching directly solvable atomic questions, ensuring each state transition depends only on the current state while progressively reducing problem complexity.

This design endows AoT with two key advantages. First, AoT eliminates the need for maintaining and computing historical information when scaling computational resources. Second, these atomic questions can be seamlessly integrated into existing test-time scaling frameworks, allowing AoT to function as either a standalone framework or a plug-in enhancement for improving the overall reasoning capabilities.

In summary, our contributions are as follows:

    •

    Atom of Thoughts. We introduce AoT, a novel reasoning framework with Markov property that progressively decomposes problems into atomic units. This approach significantly reduces computational resources wasted on historical information processing, allowing the model to focus on effective reasoning during test-time scaling.
    •

    Plug-In Enhancement. The atomic questions derived by AoT can be directly integrated into existing test-time scaling methods Bi et al. (2024); Wang et al. (2023b), enhancing both their performance and cost efficiency.
    •

    Extensive Evaluation. Experiments across six benchmarks demonstrate the effectiveness of AoT both as a standalone framework and as a plug-in enhancement. AoT outperforms all baselines, and notably on HotpotQA dataset, enables gpt-4o-mini to surpass reasoning models: o3-mini by 3.4% and DeepSeek-R1 by 10.6%.

2 Related Work
2.1 Reasoning Framework

Chain-of-Thought Wei et al. (2022) prompting has emerged as a fundamental technique for enhancing LLMs’ reasoning. Decomposition methods like Least-to-Most Zhou et al. (2023) and Plan-and-Solve Wang et al. (2023a) prompting parse complex problems into sequential subtasks. Iterative optimization approaches like Self-Refine Madaan et al. (2023), Step-Back Zheng et al. (2024) prompting and Progressive-Hint Prompting Zheng et al. (2023) refine solutions through cyclic feedback or abstraction. Multi-path aggregation techniques like Self-Consistency CoT Wang et al. (2023b) and LLM-Blender Jiang et al. (2023) further improve reasoning reliability by multi-trajectory consensus.

More sophisticated frameworks structure the representation of reasoning space through dedicated formalisms: Tree of Thoughts  Yao et al. (2023) enables systematic exploration of multiple reasoning paths, while Graph of Thoughts Besta et al. (2024) represents reasoning processes as dynamic graphs with backtracking mechanisms. Addressing fundamental limitations in resampling-based paradigms, Thought Space Explorer Zhang and Liu (2024) strategically explores under-sampled regions of the solution space. These frameworks serve as universal augmentation of LLMs reasoning, enhancing their capacity across various domains, with their principles being widely adopted in agentic workflows for code generation, question answering, and data science applications Hong et al. (2024b); Zhang et al. (2024a); Hong et al. (2024a); Zhang et al. (2025); Xiang et al. (2025); Zhang et al. (2024b).
2.2 Test-time Scaling

Test-time scaling approaches have demonstrated the value of extended computation during inference. Supervised fine-tuning on long chain-of-thought traces has proven effective at enhancing models’ capabilities to conduct extended reasoning Yeo et al. (2025); Yao et al. (2025). Building on this foundation, reinforcement learning methods have enabled models to automatically learn optimal inference expansion strategies, allowing for adaptive scaling of the reasoning process Kimi et al. (2025); Zeng et al. (2025); DeepSeek-AI (2025). Framework-based approaches have further expanded these capabilities by extending inference through external systems, incorporating techniques like verification, budget forcing, and ensemble methods Zhang et al. (2024a); Saad-Falcon et al. (2024); Chen et al. (2024). These complementary approaches demonstrate how strategic use of additional computation during inference through learned behaviors, automated scaling, and system-level interventions can substantially enhance model performance.

However, these approaches universally maintain extensive historical information throughout the reasoning process, leading to computational inefficiency and potential interference with effective reasoning. In contrast, AoT introduces a Markovian perspective that eliminates the need for historical dependency tracking, enabling more efficient resource allocation while maintaining compatibility with existing test-time scaling methods.
3 An Overview of AoT

This section presents an overview of AoT from a probabilistic modeling perspective. We first examine how traditional reasoning chains work and then introduce our dependency-based graph structures and their contraction mechanisms to enhance the modeling capability of reasoning processes.
3.1 Reasoning Chain

Chain-of-Thought (CoT) prompting enables LLMs to progressively propose intermediate thoughts Ti when solving a problem. As discussed earlier, this approach requires maintaining a complete reasoning history, which can be formalized as a probabilistic sampling procedure:
	A∼p⁢(A|𝒯,Q0)⁢∏i=0Np⁢(Ti|𝒯<i,Q0) 		(1)

Here, 𝒯={T0,T1,…,TN} represents the sequence of intermediate thoughts generated by the LLM. Each thought Ti depends on the previous thoughts 𝒯<i and the initial question Q0.

To explore chain-based methods with different node definitions, Least-to-Most Zhou et al. (2023) replaces the intermediate thoughts Ti with subquestions Qi, resulting in a different formulation of the reasoning chain:
	A∼p⁢(A|𝒬)⁢∏i=0Np⁢(Qi|𝒬<i) 		(2)

where 𝒬={Q0,Q1,…,QN} is the sequence of subquestions.

In an ideal scenario where the reasoning chain 𝒬 exhibits the Markov property, each subquestion Qi+1 would only depend on its immediate predecessor Qi, similar to how humans naturally solve complex problems by resolving independent subquestions and reformulating simplified states. This leads to:
	A∼p⁢(A|QN)⁢∏i=0Np⁢(Qi+1|Qi) 		(3)

However, achieving true Markov property in real-world reasoning tasks is challenging. We adopt the subquestion-based node structure from reasoning chains as states while exploring a two-phase state transition mechanism consisting of decomposition and contraction to address this challenge.
3.2 Dependency Directed Acyclic Graph

AoT utilizes temporary DAG structures to decompose the current question, unlike existing methods that maintain complex dependencies throughout the reasoning process. This DAG structure serves as a scaffold during state transitions, providing rich structural information to guide the complete state transition process, specifically functioning as the decomposition phase to facilitate the subsequent contraction phase.

The DAG 𝒢 is defined as:
	𝒢=(𝒬,E),𝒬={Qi}i=1n,E⊆𝒬×𝒬 		(4)

In our DAG definition, nodes represent subquestions Qi, and edges (Qj,Qi) indicate that Qj contains necessary information for solving Qi. A major challenge in constructing Markov processes stems from the dependencies of various information in complex reasoning scenarios, and this definition provides structural information for identifying dependencies through rule-based determination.

Based on their dependency relationships, all subquestion nodes can be categorized into two types:

Independent subquestions 𝒬ind (nodes without incoming edges):
	𝒬ind={Qi∈𝒬∣∄⁢Qj∈𝒬,(Qj,Qi)∈E} 		(5)

Dependent subquestions 𝒬dep (nodes with incoming edges):
	𝒬dep={Qi∈𝒬∣∃Qj∈𝒬,(Qj,Qi)∈E} 		(6)

The key assumption of acyclicity in our DAG is guaranteed by this edge definition: since subquestions are generated following natural language order, any subquestion Qi can only depend on previously generated subquestions Q<i. Even in the maximally connected case where each subquestion links to all its predecessors, acyclicity is maintained, as any additional edges would create cycles by connecting to future nodes while violating the natural language order.
3.3 Contraction

The contraction phase transforms the temporary DAG structure into the next atomic state while preserving the Markov property. To ensure this Markov process is meaningful, we must maintain state atomicity while ensuring progress in the reasoning process. As the reasoning progresses, new conclusions and information are continuously derived, necessitating the selective discarding of information to maintain atomic states. AoT addresses this by treating results from 𝒬ind as either given conditions or eliminated process information, while contracting 𝒬dep into an independent question as the next state. This contracted question maintains solution equivalence to Qi, ensuring the reasoning process stays on track.

The reasoning process is formally described in Algorithm 1, which shows how AoT iterates through decomposition and contraction steps. This iterative process continues until it reaches a maximum number D, which is assigned by the depth of the first generated graph 𝒢0 to prevent infinite decomposition. The process can be formalized as:
	A∼p⁢(A|QD)⁢∏i=0Dp⁢(Qi+1|𝒢i)⁢p⁢(𝒢i|Qi) 		(7)
Algorithm 1 Algorithm of AoT
0:  Initial question Q0
0:  Final answer A
1:  Iteration counter i←0
2:  max depth D←None
3:  while i<D or D is None do
4:     𝒢i←decomposeLLM⁢(Qi) // Generate dependency DAG
5:     if D is None then
6:        D←GetMaxPathLength⁢(𝒢i) // Rule-based path length calculation
7:     end if
8:     𝒬i⁢n⁢d←{Qi∈𝒬∣∄⁢Qj∈𝒬,(Qj,Qi)∈E}
9:     𝒬d⁢e⁢p←{Qi∈𝒬∣∃Qj∈𝒬,(Qj,Qi)∈E}
10:     Qi+1←contractLLM⁢(𝒬i⁢n⁢d,𝒬d⁢e⁢p) // Contract subquestions into a independent question
11:     i←i+1
12:  end while
13:  A←solveLLM⁢(QD) // Generate final answer
14:  return  A
4 The Design Details of AoT

This section details the implementation of AoT’s core components: decomposition and contraction, which together form one iteration of state transition in the Markov reasoning process, as illustrated in Figure 2. Through structured decomposition and principled contraction, our approach establishes a foundation for iterative reasoning that can flexibly integrate with other methods while balancing computational efficiency and reasoning depth.
4.1 Decomposition
Dependency Directed Acyclic Graph.

Addressing the challenge of excessive historical information maintenance, our decomposition phase introduces an efficient dependency extraction mechanism that only temporarily captures rich structural information, which provides the foundation for subsequent simplification. This process starts with decomposing the current question into granular subquestions, then leverages LLMs’ zero-shot capabilities to efficiently identify inter-question dependencies. The dependency extraction is achieved through a JSON-formatted LLM invocation that progressively labels each subquestion’s dependencies by indexing its upstream questions (see Appendix B.2 for annotation prompt templates).
4.2 Contraction
Subquestions Contracting.

Based on the dependency relationships identified in DAG structure, AoT performs contraction through a single LLM invocation. This process constructs an independent contracted question by selectively integrating information from independent subquestions as known conditions and incorporating the descriptions of current dependent subquestions into the main body. This process maintains answer equivalence throughout the Markov chain while continuously eliminating the test-time of solved independent subquestions in past iterations when solving the contracted question independently. The elimination of the dependency relationships from independent subquestions and the generated contracted question facilitates the transmission of key information that causes dependency. (see Appendix B.3 for contraction prompt templates).
Markov Property Maintenance.

Through this contraction process, AoT effectively eliminates redundant information in historical reasoning steps to reduce the test-time required for solving questions in subsequent states. The contraction mechanism ensures that each state generated in the process depends only on its immediate predecessor, preserving the Markov property while progressively simplifying inherent complexity of the question in the current state.
4.3 Integration
Iterative Process.

The pipeline of AoT operates through an iterative process where each state transition step involves question decomposition followed by contraction. The contracted question from each iteration serves as the input for the next decomposition phase. As the number of iterations increases, the test-time scales up in an attempt to achieve more robust and effective reasoning.
Termination Mechanism.

To optimize test-time efficiency, AoT incorporates an automated termination mechanism that uses LLM evaluation to assess solution quality through answer comparison. After each contraction step, an LLM examines three key elements: the execution results of the original question Qi, the decomposed DAG structure 𝒢⁢i, and the independent execution results of contracted question Qi+1. The LLM synthesizes these elements to generate a comprehensive answer for Qi. If this synthesized answer demonstrates consistency with the answer produced by Qi+1, the iterative process continues. Upon termination, AoT combines the current contracted question with the union of independent subquestions 𝒬d⁢e⁢p=⋃j=1i𝒬d⁢e⁢pj accumulated from all previous iterations to form a complete solution to the initial question Q0. This structure provides a solution composed entirely of independent questions, maintaining semantic independence of each subquestion while ensuring completeness of whole solution.
Integration Through Configurable Termination.

Building upon this termination mechanism, AoT enables seamless integration with existing test-time scaling methods by allowing any intermediate state to serve as an entry point. This flexibility comes from AoT’s configurable termination strategy - often just a single decomposition-contraction cycle - before passing this simplified question to other methods (refer to the right portion of Figure 2). This approach leverages AoT’s structural optimization capabilities as a preprocessing step while allowing other methods to operate on a more manageable question. The contracted question passed to subsequent methods maintains answer equivalence with the original question while AoT’s initial structural simplification helps redirect computational resources towards more direct and effective reasoning. The seamless transition between methods is facilitated by the atomic state representation in our Markov process, ensuring that essential question characteristics are preserved while unnecessary historical information is eliminated.
Table 1: Performance Comparison Across Tasks (%). We evaluate three variants: the base version (AoT), a version integrated with FoT (AoT (d=1) + FoT(n=2)), and a computationally intensive version (AoT ∗) that uses LLM to select the optimal answer from three runs. Results are reported as exact match accuracy for MATH, GSM8K, BBH, and MMLU-CF, and F1 scores for HotpotQA and LongBench.
Method 	MATH 	GSM8K 	BBH 	MMLU-CF 	HotpotQA 	LongBench 	Avg.
CoT 	78.3 	90.9 	78.3 	69.6 	67.2 	57.6 	73.7
CoT-SC (n=5) 	81.8 	92.0 	83.4 	71.1 	66.2 	58.6 	75.5
Self-Refine 	78.7 	91.7 	80.0 	69.7 	68.3 	58.2 	74.4
Analogical Prompting 	65.4 	87.2 	72.5 	65.8 	64.7 	52.9 	68.1
AFlow 	83.0 	93.5 	76.0 	69.5 	73.5 	61.0 	76.1
FoT (n=8) 	82.5 	94.0 	82.4 	70.6 	66.7 	59.1 	75.9
AoT (d=1) + FoT (n=2) 	82.6 	94.2 	82.2 	69.7 	67.6 	58.4 	75.8
AoT (Ours) 	83.6 	95.0 	86.0 	70.9 	80.6 	68.5 	80.8
AoT ∗ (Ours) 	84.9 	95.1 	87.4 	71.2 	81.0 	68.8 	81.4
5 Experiments

We conduct comprehensive experiments to examine AoT through extensive benchmark evaluation on six standard datasets, reasoning models comparison, test-time optimization experiments, and ablation studies. Our main results demonstrate consistent improvements across different reasoning tasks, with significant gains especially in multi-hop reasoning. Through comparison with state-of-the-art reasoning models, we show AoT’s effectiveness as a general framework. Our test-time optimization experiments further validate AoT’s adaptability and efficiency. Finally, ablation studies on key components like DAG structure and decomposition mechanism confirm the essentiality of our design choices.
5.1 Experimental Setup
Datasets.

We evaluate AoT using gpt-4o-mini-0718 as the backbone model, chosen for its strong performance-efficiency trade-off. Our evaluation covers four categories of reasoning tasks: mathematical reasoning (MATH Hendrycks et al. (2021) with numerical answers and GSM8K Cobbe et al. (2021)), knowledge-intensive reasoning (MMLU-CF Zhao et al. (2024)), logical reasoning (multiple-choice subsets of BBH Suzgun et al. (2023), see Appendix D.1 for details), and multi-hop reasoning (HotpotQA Yang et al. (2018) and LongBench Bai et al. (2024) which test models’ ability to connect information across multiple contexts). We use the first 1,000 examples from each dataset’s test set, except for GSM8K where we use its complete test set (1,319 examples) and LongBench where we use the combined MuSiQue Trivedi et al. (2022) and 2WikiMultiHopQA Ho et al. (2020) subsets (400 examples).
Baselines.

Our baselines include classical prompting methods (Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC, n = 5), Self-Refine, and Analogical Reasoning Yasunaga et al. (2024)) and advanced reasoning frameworks (agentic workflow AFlow Zhang et al. (2024a) and Forest of Thought (FoT)). For FoT, we implement it using Tree of Thoughts with branch number b = 3, chosen for its generalizability across diverse tasks. All experiments are averaged over three runs, with detailed reproduction settings in Appendix D.
5.2 Experimental Results and Analysis.
Main Results

As shown in Table 1, AoT demonstrates consistent improvements across different reasoning tasks. AoT achieves strong performance on mathematics tasks, with AoT ∗ reaching 84.9% on MATH and 95.1% on GSM8K (+1.9% over AFlow on MATH, +1.1% over FoT(n=8) on GSM8K). The most notable improvements are in multi-hop QA tasks, where our base version achieves 80.6% F1 score on HotpotQA (+7.1% over AFlow). Similar improvements on LongBench (68.8%, +7.5% over AFlow) further demonstrate the effectiveness of AoT’s atomic state representation in long context scenarios.
Reasoning Models Comparison Results.
Table 2: Comparison of Reasoning Model Performance on Multi-hop QA Tasks. Results show F1 scores and Hit rates (F1 > 0) for HotpotQA and LongBench across different models.
Method 	HotpotQA 	LongBench
	F1 	Hit 	F1 	Hit
CoT 	QwQ 	68.1 	82.4 	52.7 	65.6
DeepSeek-R1 	70.0 	85.5 	56.0 	69.9
o3-mini 	77.2 	88.3 	55.3 	70.0
AoT 	gpt-4o-mini 	80.6 	89.8 	60.5 	69.3
o3-mini 	81.4 	91.4 	63.3 	72.1

We compare AoT with several reasoning models, including QwQ-32B-Preview Qwen-Team (2024), DeepSeek-R1 DeepSeek-AI (2025), and o3-mini-2025-01-31OpenAI (2025). Notably, o3-mini demonstrates remarkable raw performance with a 77.2% F1 score on HotpotQA, surpassing our previous best baseline AFlow (73.5% F1) in the main experiments, highlighting its strength as a foundation model. When integrated into our framework, even a relatively modest model like gpt-4o-mini achieves an impressive 80.6% F1 score. Furthermore, employing o3-mini as the backbone of AoT leads to exceptional results: the F1 score increases to 81.4% and the Hit rate reaches 91.4% on HotpotQA. On the LongBench subset, our framework with o3-mini achieves a 63.3% F1 score and 72.1% Hit rate, establishing new state-of-the-art performance across all metrics. Due to the computational constraints and stability considerations, we evaluated on the first 100 examples from the Musique subset of LongBench, which may result in slightly higher scores compared to our main experiments in Table 1.
Test-Time Optimization Results.
Refer to caption
Figure 3: Performance scaling with transition times on MATH dataset. Darker blue indicates larger sample sizes at shallower depths, as most problems are solved with fewer decomposition steps.

We investigate the test-time scaling behavior of AoT through two sets of experiments. First, as shown in Figure 3, we analyze the performance scaling of AoT on MATH dataset. Unlike the dynamic iteration limit determined by problem-specific graph structures described in Section 3, here we set a uniform maximum of 5 iterations to explicitly examine the depth-wise scaling behavior. Since each iteration produces an evaluable solution, we can track performance across different iteration depths. All 1000 test samples naturally generate solutions at depth 1, while fewer samples proceed to deeper iterations (dropping to 207 at depth 5), as many problems achieve satisfactory solutions at earlier depths. The results demonstrate that AoT exhibits consistent accuracy improvements from 83.2% to 92.7% as the iteration depth increases, with the performance gains gradually tapering. This pattern suggests that while deeper iterations continue to benefit overall performance, many problems can be effectively solved with fewer iterations, providing a natural trade-off between computational cost and solution quality.
Refer to caption
Figure 4: Performance comparison on MATH dataset showing computational efficiency. The green line shows FoT scaling with varying tree numbers (2k,k=0,1,2,…), while the gray trend line (representing other baseline methods) together demonstrate the trade-off between performance gains and computational costs. AoT (d=1) combined with FoT(n=2) achieves slightly better performance to standalone FoT(n=8) while requiring substantially less computation.

In our second experiment (Figure 4), we examine the effectiveness of AoT as a plug-in for existing test-time scaling methods. When integrated with FoT, AoT demonstrates promising efficiency. This efficiency gain stems from how AoT restructures the reasoning process: by iteratively solving sub-problems and using them as known conditions for subsequent steps, it eliminates redundant derivations. This leads to substantially reduced test-time demands in the FoT phase while achieving slightly better performance, demonstrating how our approach can systematically optimize existing test-time scaling methods.
Cost Analysis.

Through analyzing computational efficiency as shown in Figure 4, our AoT achieves superior efficiency by reaching competitive performance at significantly lower computational costs compared to existing methods. This enhanced efficiency can be attributed to our atomic state representation that preserves only necessary information while eliminating redundant computations. Notably, AoT demonstrates the steepest performance-to-cost ratio among all compared methods, indicating it achieves the highest marginal improvement in accuracy per unit of computational investment.
Ablation Study.
Table 3: Ablation Study on AoT Components (%). Removing the decomposition phase causes notable performance drops, while removing the DAG structure but keeping decomposition leads to even larger degradation.
Method 	MATH 	GSM8K
AoT (Full) 	83.6 	95.0
AoT w/o Decomposition 	82.9 	94.8
AoT w/o DAG Structure 	82.7 	94.3

We conduct ablation studies to analyze the contribution of key components in AoT. As shown in Table 3, removing the decomposition phase (i.e., no extracted independent or dependent sub-problems as guidance) causes notable performance drops, while removing the DAG structure but keeping the decomposition phase (i.e., only extracting the first semantically independent sub-problem as guidance) leads to even larger degradation. Without decomposition structure, the LLM struggles to capture crucial dependencies between subquestions in the contraction phase, resulting in contracted questions that often contain redundant information. Moreover, providing single sub-problem guidance without proper structural information disrupts the parallel relationships between sub-problems. This reveals a critical insight: imperfect structural guidance can be more detrimental than no guidance at all (see Appendix C.1 for examples).
6 Conclusion

In this paper, we introduced Atom of Thoughts (AoT), a novel framework that transforms complex reasoning processes into a Markov process of atomic questions. By implementing a two-phase transition mechanism of decomposition and contraction, AoT eliminates the need to maintain historical dependencies during reasoning, allowing models to focus computational resources on the current question state. Our extensive evaluation across diverse benchmarks demonstrates that AoT serves effectively both as a standalone framework and as a plug-in enhancement for existing test-time scaling methods. These results validate AoT’s ability to enhance LLMs’ reasoning capabilities while optimizing computational efficiency through its Markov-style approach to question decomposition and atomic state transitions.
7 Limitations

A key limitation of AoT lies in its Markov state transition process without a well-designed reflection mechanism. When the initial DAG decomposition fails to properly model parallel relationships between subquestions or captures unnecessary dependencies, it can negatively impact subsequent contraction and reasoning process, a scenario that occurs frequently in practice. The framework currently lacks the ability to detect and rectify such poor decompositions, potentially leading to compounded errors in the atomic state transitions. This limitation suggests the need for future research into incorporating effective reflection and adjustment mechanisms to improve the robustness of DAG-based decomposition
